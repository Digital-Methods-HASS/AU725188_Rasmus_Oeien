---
title: "GOTsentimentAnalasys"
author: "RasmusOeien"
date: "2024-04-03"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = TRUE,
                      message = TRUE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!

```

### Get the GOT data:
```{r get-document}
GOT_path <- here("data","got.pdf")
GOT_text <- pdf_text(GOT_path)
```

### Take a page out of the text 
```{r single-page}
GOT_p9 <- GOT_text[9]
GOT_p9
```

### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
GOT_df <- data.frame(GOT_text) %>% 
  mutate(text_full = str_split(GOT_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.
# Although, this time round, it is working for me with \n alone. Wonders never cease.



```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
GOT_tokens <- GOT_df %>% 
  unnest_tokens(word, text_full)
GOT_tokens

# See how this differs from `ipcc_df`
# Each word has its own row!
```
Let's count the words!
```{r count-words}
GOT_wc <- GOT_tokens %>% 
  count(word) %>% 
  arrange(-n)
GOT_wc
```

### Remove stop words:

All of the most commonly used words like "and" filtered out so that there is meaningfull data to analyse.

See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons.

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
GOT_stop <- GOT_tokens %>% 
  anti_join(stop_words) %>% 
  select(-GOT_text)
```


Now check the counts again: 
```{r count-words2}
GOT_swc <- GOT_stop %>% 
  count(word) %>% 
  arrange(-n)
GOT_swc
#GOT_swc is then the wordlist without the stopwords
```


What if we want to get rid of all the numbers (non-text) in `GOT`?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

GOT_no_numeric <- GOT_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)


```{r wordcloud-prep}
# Tells how many unique words there are, expressed below 
length(unique(GOT_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
GOT_top100 <- GOT_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```


```{r wordcloud}
GOT_cloud <- ggplot(data = GOT_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

GOT_cloud
```
That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = GOT_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```
### Sentiment analysis

First, check out the ‘sentiments’ lexicon. From Julia Silge and David Robinson (https://www.tidytextmining.com/sentiment.html):

“The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.  The bing lexicon categorizes words in a binary fashion into positive and negative categories. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.  All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon."

Let's explore the sentiment lexicons. "bing" is included, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to download.

**WARNING:** These collections include very offensive words. I urge you to not look at them in class.

"afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```
bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```
Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```
### Sentiment analysis with afinn: 

First, bind words in `GOT_stop` to `afinn` lexicon:
```{r bind-afinn}
GOT_afinn <- GOT_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
GOT_afinn_hist <- GOT_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = GOT_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```


Investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
GOT_afinn2 <- GOT_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(GOT_afinn2$word)

# Count & plot them
GOT_afinn2_n <- GOT_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = GOT_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

# Here we see revive as the most common word with a value of 2. Though having read through the book i would not associate the revival of undead corpses to be positive 
```

Look back at the GOT text, and search for "confidence." Is it typically associated with emotion, or something else? 

We learn something important from this example: Just using a sentiment lexicon to match words will not differentiate between different uses of the word...(ML can start figuring it out with context, but we won't do that here).

Or we can summarize sentiment for the report: 
```{r summarize-afinn}
GOT_summary <- GOT_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

Here we see that the mean value ends up at -0.542 which means that there is a slight negative tilt in the sentiment.

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
GOT_nrc <- GOT_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r check-exclusions}
GOT_exclude <- GOT_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(GOT_exclude)

# Count to find the most excluded:
GOT_exclude_n <- GOT_exclude %>% 
  count(word, sort = TRUE)

head(GOT_exclude_n)
```

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-bing}
GOT_nrc_n <- GOT_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = GOT_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```

Or count by sentiment *and* word, then facet:
```{r count-nrc}
GOT_nrc_n5 <- GOT_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

GOT_nrc_gg <- ggplot(data = GOT_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
GOT_nrc_gg

# Save it
ggsave(plot = GOT_nrc_gg, 
       here("figures","GOT_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

Wait, so "confidence" is showing up in NRC lexicon as "fear"? Let's check:
```{r nrc-confidence}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "confidence")

# Yep, check it out:
conf
```

So we find that the general sentiment of Game of thrones is slightly negative. That tracks with what i would expect to find in a book about lies and backstapping. There are even words that in a reading of the book would be associated with negative actions that are counted in the lexica as positive and that skews our results. So in reality the sentiment is probably even more negative.











